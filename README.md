
# Awesome-LLM-WebSearch-RAG [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT)

- ❤❤❤  We provide a comprehensive list of LLM Websearch RAG, focusing on developing and optimizing web-scale Retrieval-Augmented Generation systems tailored for understanding up-to-date vision knowledge.


## Table of Content

- [Awesome-Websearch-RAG](#Awesome-LLM-Websearch-RAG)
  - [RAG Survey](#RAG-Survey)
  - [Enhanced Retrieval](#Enhanced-Retrieval)
  - [Video RAG](#Video-RAG)
  - [Websearch RAG](#Websearch-RAG)

## RAG-Survey

|  Date |       Keywords       |    Institute (first)   | Paper                                                                                                                                                                               | Publication | Code | Project | 
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------:| :---------: 
| 2024-11-26 | Survey | Sun Yat-sen | [Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey](https://arxiv.org/abs/2411.17558) | Arxiv 2024 |   |  | 
| 2024-10-28 | Survey | CMU | [A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2410.12837) | Arxiv 2024 |   |  | 
| 2024-09-23 | Survey | Microsoft | [Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely](https://arxiv.org/abs/2409.14924) | Arxiv 2024 |   |  | 
| 2024-06-17 | Survey | HK PolyU | [A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2405.06211) | KDD 2024 |  [Code](https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/) | [Project](https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/) | 




## Enhanced-Retrieval
|  Date |       Keywords       |    Institute (first)   | Paper                                                                                                                                                                               | Publication | Code | Project |  Others | 
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------:| :---------: | :---------: 
| 2024-12-07 | RAGraph | PKU | [RAGraph: A General Retrieval-Augmented Graph Learning Framework](https://arxiv.org/abs/2410.23855) | NIPS 2024 | [Code](https://github.com/Artessay/RAGraph/)  |  |   | | 
| 2024-10-28 | SubgraphRAG | Georgia Tech | [Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2410.20724) | Arxiv 2024  |  [Code](https://github.com/Graph-COM/SubgraphRAG) |  |  | 
| 2024-10-9 | Text Proxy | HFUT | [Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N 1-to-1 Relationships for Text-Video Retrieval](https://arxiv.org/abs/2410.06618) | AAAI 2025 | [Code](https://github.com/musicman217/Text-Proxy/tree/main)| | |
| 2024-10-08 | VisRAG | Tsinghua | [VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents](https://arxiv.org/abs/2410.10594) | Arxiv 2024 |  [Code](https://github.com/openbmb/visrag) |  |   | | 
| 2024-07-11 | Speculative RAG | UCSD | [Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting](https://arxiv.org/abs/2407.08223) | Arxiv 2024  |  |   | | 
| 2024-06-03 | RGNN-Ret | HKUST | [Graph Neural Network Enhanced Retrieval for Question Answering of LLMs](https://arxiv.org/abs/2406.06572) | Arxiv 2024  |  |   | | 
| 2024-05-26 | GRAG | Emory | [GRAG: Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2405.16506) | Arxiv 2024 |   |  |   | | 
| 2024-01-26 | CRAG | USTC | [Corrective Retrieval Augmented Generation](https://arxiv.org/pdf/2401.15884) | Arxiv 2024 |  [Code](https://github.com/HuskyInSalt/CRAG)  |  |   | | 



## Video-RAG

|  Date |       Keywords       |    Institute (first)   | Paper                                                                                                                                                                               | Publication | Code | Project |  Others | 
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------:| :---------: | :---------:
| 2024-11-30 | Video-RAG | Xiamen U | [Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension](https://arxiv.org/abs/2411.13093) | Arxiv 2024 |  [Code](https://github.com/Leon1207/Video-RAG-master) |  |   | | 
| 2024-07-22 |  | Microsoft | [An Empirical Comparison of Video Frame Sampling Methods for Multi-Modal RAG Retrieval](https://arxiv.org/abs/2408.03340) | Arxiv 2024 | |  |   | 
| 2024-07-17 | EchoSight | SJTU | [EchoSight: Advancing Visual-Language Models with Wiki Knowled](https://arxiv.org/abs/2407.12735) | EMNLP 2024 |  [Code](https://github.com/Go2Heart/EchoSight) | [Project](https://go2heart.github.io/echosight/)  |   | | 
| 2024-06-15 |  | Stanford | [Leveraging Lightweight AI for Video Querying in a RAG Framework](https://www.semanticscholar.org/paper/Leveraging-Lightweight-AI-for-Video-Querying-in-a-Chun-Hsu/8724c1ebf16e4342edb9f5fd2fb94e7f13818978) |  | |  |   | 
| 2024-06-15 |  | Moments Lab | [Towards Retrieval Augmented Generation over Large Video Libraries](https://arxiv.org/abs/2406.14938) | HSI 2024 | |  |   | 
| 2024-05-27 |  | Snap Inc. | [Video Enriched Retrieval Augmented Generation Using Aligned Video Captions](https://arxiv.org/abs/2405.17706) | SIGIR 2024 Workshop| |  |   | 
| 2024-04-18 | iRAG | NEC Lab | [iRAG: Advancing RAG for Videos with an Incremental Approach](https://arxiv.org/abs/2404.12309) | CIKM 2024 |  |  |   | 
| 2023-12-18 | ViTA | NEC Lab | [ViTA: An Efficient Video-to-Text Algorithm using VLM for RAG-based Video Analysis System](https://openaccess.thecvf.com/content/CVPR2024W/MAR/papers/Arefeen_ViTA_An_Efficient_Video-to-Text_Algorithm__using_VLM_for_RAG-based_CVPRW_2024_paper.pdf) | CVPR 2024 |  |  |   | | 



## Websearch-RAG

|  Date |       Keywords       |    Institute (first)   | Paper                                                                                                                                                                               | Publication | Code | Project |  Others | 
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------:| :---------: | :---------:
| 2024-12-08 |  OmniSearch | Alibaba | [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://arxiv.org/abs/2411.02937) | Arxiv 2024 |  [Code](https://github.com/Alibaba-NLP/OmniSearch) | [Project](https://github.com/Alibaba-NLP/OmniSearch?tab=readme-ov-file) | |
| 2024-11-29 | 01.AI | HKUST | [Zero-Indexing Internet Search Augmented Generation for Large Language Models](https://arxiv.org/abs/2411.19478) | Arxiv 2024 |   | [Project](https://www.01.ai/) | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/blob/main/assets/01.AI/01/Zero-Indexing%20Internet%20Search%20Augmented%20Generation%2015ccd9a0c43a806fb52bc706798545a8.md) |
| 2024-11-29 | Auto-RAG | ICT/CAS | [Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2411.19443) | Arxiv 2024 |  [Code](https://github.com/ictnlp/Repository-for-the-forthcoming-work) | | |
| 2024-11-25 | ReflectiVA | Unimore | [Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2411.16863) | Arxiv 2024 | [Code](https://github.com/aimagelab/ReflectiVA) | | |
| 2024-11-22 | mR2AG | CASIA | [mR2AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA](https://arxiv.org/abs/2411.15041) | Arxiv 2024 | |  |   | 
| 2024-11-11 | Invar-RAG | CityU HK | [Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation](https://arxiv.org/abs/2411.07021) | Arxiv 2024 |    |   | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/blob/main/assets/invar/Invar-RAG%20Invariant%20LLM-aligned%20Retrieval%20for%20Bett%2015dcd9a0c43a8038ab69fec551800b82.md) |
| 2024-11-05 | HtmlRAG | RUC | [HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems](https://arxiv.org/abs/2411.02959) | Arxiv 2024 |  [Code](https://github.com/plageon/HtmlRAG) |  | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/blob/main/assets/01.AI/01/Zero-Indexing%20Internet%20Search%20Augmented%20Generation%2015ccd9a0c43a806fb52bc706798545a8.md) |
| 2024-10-28 | VSA | MMLab | [Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines](https://arxiv.org/abs/2410.21220) | Arxiv 2024 |  [Code](https://github.com/cnzzx/VSA) | [Project](https://cnzzx.github.io/VSA/) | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/blob/main/assets/vsa/Vision%20Search%20Assistant%20Empower%20Vision-Language%20Mo%2015dcd9a0c43a80b8bd7bc53baedec983.md) |
| 2024-10-9 | AutoFeedback | NUDT | [AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation](https://arxiv.org/abs/2410.06943) | Arxiv 2024 | | | |
| 2024-10-9 | WeKnow-RAG | Tsinghua | [WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs](https://arxiv.org/abs/2408.07611) | KDD 24 Workshop |  |  | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/tree/main/assets/WeKnow-RAG) |
| 2024-06-19 | FoRAG | Ant | [FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering](https://arxiv.org/abs/2406.13779) | KDD 24 |  | [Huggingface](https://huggingface.co/forag) | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/tree/main/assets/WeKnow-RAG) |
| 2024-05-23 | SearchLVLMs | SHAI Lab | [SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge](https://openreview.net/forum?id=leeosk2RAM&referrer=%5Bthe%20profile%20of%20Chuanhao%20Li%5D(%2Fprofile%3Fid%3D~Chuanhao_Li2)) | NeurIPS 2024 poster |  [Code](https://github.com/NeverMoreLCH/SearchLVLMs) | [Project](https://nevermorelch.github.io/SearchLVLMs.github.io/) | [View](https://github.com/01yzzyu/Websearch-RAG/blob/main/assets/SearchLVLMs/SearchLVLMs.md) |
| 2024-03-31 | RQ-RAG | HKUST | [RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation](https://arxiv.org/abs/2404.00610) | COLM 2024 |  [Code](https://github.com/chanchimin/RQ-RAG) | | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/tree/main/assets/rqrag) |
| 2024-03-15 | DRAGIN | Tsinghua | [DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models](https://arxiv.org/abs/2403.10081) | ACL 2024 |  [Code](https://github.com/oneal2000/DRAGIN/tree/main) |  | [View](https://github.com/01yzzyu/Awesome-LLM-Websearch-RAG/blob/main/assets/dragin/DRAGIN%20Dynamic%20Retrieval%20Augmented%20Generation%20base%2015dcd9a0c43a80a3b92fddfd32d1a036.md) |
| 2023-12-18 | CLOVA | PKU | [CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update](https://arxiv.org/abs/2312.10908) | CVPR 2024 |  [Code](https://github.com/clova-tool/CLOVA-tool) | [Project](https://clova-tool.github.io/) | |
| 2023-10-17 | Self-RAG | UW | [Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections](https://arxiv.org/abs/2312.10908) | ICLR 2024 oral |  [Code](https://github.com/AkariAsai/self-rag) | [Project](https://selfrag.github.io/) |  |
| 2023-04-19 | Chameleon | UCLA | [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://openreview.net/forum?id=leeosk2RAM&referrer=%5Bthe%20profile%20of%20Chuanhao%20Li%5D(%2Fprofile%3Fid%3D~Chuanhao_Li2)) | NeurIPS 2023 poster |  [Code](https://github.com/lupantech/chameleon-llm) | [Project](https://chameleon-llm.github.io/) |  |
| 2023-03-20 | MM-REACT | Microsoft | [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381) | CVPR 2023 |  [Code](https://github.com/microsoft/MM-REACT) | [Project](https://multimodal-react.github.io/) |   |
| 2023-03-19 | ViperGPT | Columbia | [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128) | CVPR 2023 |  [Code](https://github.com/cvlab-columbia/viper) | [Project](https://viper.cs.columbia.edu/) |  |
| 2023-03-08 | Visual ChatGPT | Microsoft | [Visual ChatGPT: Talking, drawing and editing with visual foundation models](https://arxiv.org/abs/2303.04671) | Arxiv 2023 |  [Code](https://github.com/chenfei-wu/TaskMatrix) | [Project](https://github.com/chenfei-wu/TaskMatrix) |  |

## Video Caption Benchmark
|  Date |       Keywords       |    Institute (first)   | Paper                                                                                                                                                                               | Publication | Code | Project |  Others | 
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------:| :---------: | :---------:
| 2016-05-02 | | Stanford | [Dense-Captioning Events in Videos](https://arxiv.org/abs/1705.00754) | Arxiv 2017 |  | [Project](https://cs.stanford.edu/people/ranjaykrishna/densevid/) | |
| 2016-11-29 | MSR-VTT | Microsoft | [MSR-VTT: A Large Video Description Dataset for Bridging Video and Language - Microsoft Research](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/) | CVPR 2016 |  | [Project](https://cove.thecvf.com/datasets/839) | |



## Benchmark

|  Date |       Keywords       |    Institute (first)   | Paper                                                                                                                                                                               | Publication | Code | Project |  Others | 
| :-----: | :------------------: | :--------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------: | :---------:| :---------: | :---------:
| 2024-12-08 | Dyn-VQA | Alibaba | [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://arxiv.org/abs/2411.02937) | Arxiv 2024 |  [Code](https://github.com/Alibaba-NLP/OmniSearch) | [Project](https://github.com/Alibaba-NLP/OmniSearch?tab=readme-ov-file) | |
| 2024-11-29 | TQA-Bench | HKUST | [TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with Scalable Context and Symbolic Extension](https://arxiv.org/abs/2411.19504) | Arxiv 2024 |  [Code](https://github.com/Relaxed-System-Lab/TQA-Bench) | [Project](https://github.com/Relaxed-System-Lab/TQA-Bench) | |
| 2024-11-25 | M2RAG-Bench | BIT | [Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines](https://arxiv.org/abs/2411.16365) | Arxiv 2024 |  [Code](https://github.com/maziao/M2RAG)| | |
| 2024-10-10 | MRAG-Bench | UCSD | [MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models](https://arxiv.org/abs/2410.08182) | Arxiv 2024 | [Code](https://github.com/mragbench/MRAG-Bench?tab=readme-ov-file) | [Project](https://mragbench.github.io/) | |
| 2024-10-02 | MMQA |  | [MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions](https://openreview.net/forum?id=GGlpykXDCa) | ICLR 2025 Oral | | | |
| 2024-10-07 | TableRAG | NTU(Taiwan) | [TableRAG: Million-Token Table Understanding with Language Models](https://arxiv.org/abs/2410.04739) | NIPS 2024 | [Code](https://github.com/google-research/google-research/tree/master/encyclopedic_vqa) |  |  |
| 2023-06-15 | Encyclopedic VQA | Google | [Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories](https://arxiv.org/abs/2411.19504) | ICCV 2023 |  [Code](https://github.com/google-research/google-research/tree/master/encyclopedic_vqa) |  |  |
| 2022-05-04 | WebQA | CMU | [MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding](https://arxiv.org/abs/2112.10728) | AAAI 2022 |  [Code](https://github.com/blender-nlp/MuMuQA) |   |  |
| 2022-03-28 | WebQA | CMU | [WebQA: Multihop and Multimodal QA](https://arxiv.org/abs/2109.00590) | CVPR 2022 |  [Code](https://github.com/WebQnA/WebQA_Baseline) | [Project](https://webqna.github.io/)  |  |


