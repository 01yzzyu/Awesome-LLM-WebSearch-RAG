# SearchLVLMs ( Nips 24 poster )

Augmenting Large Vision-Language Models by Searching **Up-to-Date Internet Knowledge**

- **Motivation：**
    
    **大型视觉语言模型 （LVLM） 对最新知识一无所知，**
    
    **train要大量资源，因此无法频繁更新，导致**在许多查询最新知识情况下会失败。
    
    尽管 GPT-4V [ 19 ] 和 Claude3 [ 22 ] 等商业 LVLM 具有 IAG 的能力，但支撑它们的具体机制仍未披露。
    
- **Contribution**
    
    **1.我们提出首个辅助多模态大模型对实时信息进行反馈的开源检索增强框架**SearchLVLM**。**
    
    该框架主要包括查询生成、搜索引擎调用、分层过滤三个部分。
    
    以视觉问答为例，该框架会基于问题和图片生成查询关键词，并调用搜索引擎查找相关信息，再由粗到细地对检索结果进行过滤，得到对回答该问题有帮助的信息。
    
    这些信息会以 prompt 的形式在推理阶段提供给模型，以辅助回答。
    
    经过训练，分层过滤模型可以有效且高效地从搜索引擎返回的网站中找到最有用的内容，以提示 LVLM 了解最新知识。
    
    2.为了训练模型并评估框架的性能，我们提出了一个**自动生成依赖实时信息进行回答的视觉问答数据**的管道，以构建一个称为 UDK-VQA 的数据集。
    
    3.引入了一种多模型投票机制来标记网站/内容对 VQA 样本构建训练集的有用性。实验结果表明了我们框架的有效性，其准确性比 GPT-4V 高出约 25%。
    
- **Framework：**
    
    ![image.png](SearchLVLMs%20(%20Nips%2024%20poster%20)%20152cd9a0c43a80d58470c218172a415c/image.png)
    
    如上图所示，SearchLVLMs 框架主要由三部分组成：查询生成、搜索引擎调用、分层过滤。
    
    1. 在查询生成阶段，需要对问题和图像进行**充分地理解**，以转化为适用于搜索引擎的文本查询。
        
        对于问题而言，直接使用手工设计的 prompt **调用 LLM 得到问题查询词**。
        
        对于图像而言，调用Bing视觉搜索得到包含该图像或与该图像相关的网页，提取这些网页的题目/快照的最长公共子串作为图像查询词。
        
    2. 在搜索引擎调用阶段，用户可以根据问题类型自主选择调用的搜索引擎类别。比如：对于实时性较强的新闻相关问题，可以选择调用必应新闻搜索；对于常识性问题，可以选择调用必应通用搜索。调用搜索引擎后会得到多个网页的题目、摘要和链接。
    3. 在分层过滤阶段，首先调用网页过滤器对得到的网页进行初筛，基于网页的题目和摘要对这些网页进行重排。对于排序靠前的网页，使用爬虫获取网页的文本内容，每三句切分成一个片段，使用内容过滤器对这些片段进行重排。
        
        直接使用所有网站的全部内容来增强 LVLM 是不切实际的，
        
        因为：（1） 大多数 LVLM 不擅长处理如此长的上下文。（2） 处理如此长的上下文是计算密集型和耗时的。为此，训练了一个分层过滤模型来寻找对回答问题最有用的内容，该模型首先根据每个网站的标题和片段有效地筛选网站，然后从过滤的网站中识别出最有用的内容。
        
        对于排序靠前的片段，基于 **CLIP 特征对它们进行聚类**，选择**离每个聚类中心**的最近的片段，以避免内容重复片段对大模型预测带来的误导。
        
        被选择的片段被直接拼接在一起，用于提示大模型。
        
        其中，**网页过滤器和内容过滤器是两个独立训练的 LLaVA-1.5** 模型，作用是为网页/片段进行打分——网页/片段对于回答该问题的**帮助程度**。
        
    
- **UDK-VQA 数据生成框架**
    
    ![image.png](SearchLVLMs%20(%20Nips%2024%20poster%20)%20152cd9a0c43a80d58470c218172a415c/image%201.png)
    
    为了训练这两个过滤器 and 测试大模型对实时信息的反馈能力，
    
    五个步骤：查询搜集、问题生成、图像分配、伪标注生成、人为验证。
    
    1. 查询搜集。查询搜集主要包括两方面，一方面是从谷歌每日搜索趋势上爬取热门搜索词，另一方面是人为搜集一些热门搜索词来对前者进行补充。
    
    2. 问题生成。我们首先根据搜集到的搜索词调用搜索引擎得到相关的新闻，将新闻内容进行切分，得到多个内容片段。然后，我们要求GPT-3.5 根据内容片段自问自答，得到<问题，答案>的集合。
    
    3. 图像分配。在图像分配阶段，我们会**提取出问题中的实体**，使用图片搜索引擎得到实体的图片，并将问题中的实体单词**替换为其上分位词**，与图片一起组成**视觉问答样本**。
    
    为了编写 VQA 示例，我们使用 Bing 搜索被替换实体的图像，并对它们进行聚类以减少它们中的异常值。
    
    4. 伪标注生成。为了训练网页过滤器和内容过滤器，我们需要对网页/片段进行打分。对于一个视觉问答样本和一个网页/片段，我们基于两个原则进行打分：
    
    - 如果该样本是基于该网页/片段生成的，分数为 1.0；
    - 如果该样本不是基于该网页/片段生成的，我们使用 5 个开源模型在该网页/片段下尝试回答该样本，根据模型回答的正确率进行打分。
    
    基于这样的伪标注方法，我们构造了 ~80w 样本用于训练。
    
    5. 人为验证。构造测试集时，我们对第 3 步得到的视觉问答样本进行了人为筛选，确保测试样本的正确性。为了避免训练数据和测试数据需要参考相似的实时信息，在构造训练集和测试集时，我们使用不同时间区间的谷歌每日搜索趋势来爬取热门搜索词。
    
    在此过程中，回答生成的 VQA 样本需要模型同时考虑视觉和文本信息。我们使用不同时间段的查询来抓取不同时间段的新闻，生成用于构建训练集和测试集的样本，以避免测试数据暴露在训练数据中。在训练集中，我们进一步使用多模型投票机制来标记网站的有用性和内容的有用性，并基于标签将样本与网站及其内容相结合，用于训练分层过滤模型。在测试集中，我们进行人工筛选以确保其正确性。
