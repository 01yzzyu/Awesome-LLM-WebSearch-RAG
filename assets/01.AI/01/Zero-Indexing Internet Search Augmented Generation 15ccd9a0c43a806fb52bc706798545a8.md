# Zero-Indexing Internet Search Augmented Generation for Large Language Models

- **Motivation:**
    
    这篇文章的核心问题是解决现有的基于检索增强生成（RAG，Retrieval-Augmented Generation）方法的局限性，特别是在生成需要**实时更新**的信息内容时。
    
    传统的RAG方法依赖于静态的、预处理过的语料库和索引机制(闭源，不是实时更新的)，而无法动态地融入最新的互联网信息。
    
    paper提出了一种通过搜索引擎API动态集成最新在线信息的生成pipeline，这种方法不需要维护固定语料库的索引，解决了传统RAG方法在信息时效性和内容多样性上的不足。
    
    **现有方法的问题**：
    
    •检索到的内容冗余且噪声较多。
    
    •静态索引机制的构建和维护成本高。
    
    •直接使用搜索引擎返回的原始数据可能会影响生成内容的效率和质量。
    
- **Pipeline**
    
    ![image.png](Zero-Indexing%20Internet%20Search%20Augmented%20Generation%2015ccd9a0c43a806fb52bc706798545a8/image.png)
    
    整个管道通过以下模块协同工作：
    
    1.	**PARSER-LLM**（解析器模块）：解析用户请求，判断是否需要进行互联网增强，并提取搜索关键词。
    
    2.	**搜索引擎 API 调用**：利用**提取的关键词**从搜索引擎（如 Google 或 Bing）动态检索信息。
    
    3.	**混合排名策略**：**重新排序检索到的HTML**内容，减少搜索引擎排序的**偏差，**，优选相关内容。
    
    4.	**EXTRACTOR-LLM**（信息提取模块）：从检索的HTML中提取与用户请求相关的内容，过滤无关信息。
    
    5.	**GENERATIVE-LLM**（生成模型）：将提取的上下文信息与用户请求结合扩展Prompt，生成最终的高质量响应。
    
    **1. 输入解析（Intent Parsing）**
    
    **核心组件：PARSER-LLM**
    
    **功能**：判断是否需要互联网增强生成，并提取合适的搜索关键词。
    
    - 1)  判断是否需要互联网增强生成
        
        **（1）输入请求的语义理解**
        
        **语义分析**：PARSER-LLM 使用自然语言理解（NLU）能力解析输入请求，确定其核心意图。
        
        例如：输入为 “2024年最流行的智能手机有哪些？” 时，模型会识别该请求依赖于最新信息，而不是静态数据库中已有的内容。
        
        对比：若请求为 “牛顿第一定律是什么？” 则模型会判断该问题可以从本地知识库中获取答案，无需进行互联网增强生成。
        
        **（2）规则与条件判断**
        
        PARSER-LLM 结合规则与上下文线索进行需求判定：
        
        •	**时间敏感性**：
        
        判断请求是否涉及与时间相关的内容，例如 “今年最好的减肥方法是什么？” 需要最新数据，而 “2020年诺贝尔奖得主是谁？” 则不需要。
        
        •	**上下文覆盖范围**：
        
        •	若请求涉及最新事件（如新闻）、市场趋势（如股票信息）、实时数据（如天气），则判断需要增强生成。
        
        •	**请求模糊性**：
        
        •	如果请求中存在模糊性或需要补充上下文信息（如 “最新的AI进展”），模型会**倾向于调用互联网检索。**
        
        **（3）多任务推理**
        
        在单次推理中，PARSER-LLM 会输出以下两项结果：
        
        **是否需要互联网增强生成**（Yes/No） and 关键词
        
    - 2）**关键词提取**：在一次推理中解析出高效的搜索关键词。
        
        **特性处理**：
        
        •	**时间敏感性**：动态判断关键词是否需要加入时间信息（例如年份或日期）。
        
        •	**多语言支持**：根据输入请求的语言特性提取跨语言关键词。
        
        •	**词汇扩展**：处理未见过的专有名词或新术语，避免因词汇不足导致的检索失误。
        
    
    **2. 互联网搜索（Dynamic Search Querying）**
    
    **核心组件：标准搜索引擎API（Google/Bing）**
    
    1.	**关键词查询**：用提取的关键词向搜索引擎API发送请求。
    
    2.	**检索结果处理**：
    
    搜索引擎返回HTML格式的网页内容，初步处理这些HTML文件，标记并存储其内容。
    
    - 预处理
        
        **清理与筛选**：去掉无关的 HTML 元素和噪声内容（如广告、脚本、样式表等）。
        
        **分割与标记**：将网页内容分割成逻辑片段（如段落或句子），并为每个片段添加标记（tag）。
        
        **结构化存储**：将标记后的内容以易于访问的格式存储，方便后续的提取和处理。
        
    
    **3. 内容排序（Content Ranking）**
    
    **核心组件：混合排名策略（Mixed Ranking Strategy）**
    
    •	**功能**：重排序检索到的HTML文件，减少搜索引擎自身排序算法的偏差。
    
    1.	**分级内容排序**：
    
    •	**粗粒度排序(摘要)**：基于搜索引擎返回的摘要内容（Snippet）进行初步排序。
    
    •	**细粒度排序(正文)**：基于HTML文件正文内容的**语义相关性（余弦相似度（Cosine Similarity）** 衡量请求与HTML正文各段内容的语义相关性。**）**进行更细致的排序。
    
    2.	**结合策略**：
    
    •	生成两种排序结果（Snippet 和 Full Content）。
    
    •	每个HTML文件的最终得分取决于摘要和正文的加权结果：
    
    $$
    S_{\text{final}} = \alpha \cdot S_{\text{snippet}} + \beta \cdot S_{\text{content}}
    $$
    
    3.	**去偏机制**：
    
    •	减少因点击率等因素引入的商业偏差，提高检索结果的真实性和相关性。
    
    $$
    如果请求需要快速响应或只需粗略信息，则提高摘要的权重  \alpha
    
    $$
    
    $$
    如果请求需要深度信息，则提高正文的权重  \beta 。
    $$
    
    **4. 信息提取（Information Extraction）**
    
    **核心组件：EXTRACTOR-LLM**
    
    •	**功能**：从HTML文件中提取与用户请求高度相关的内容，剔除冗余或无关信息。
    
    •	**语义相关性提取**：基于用户请求，筛选HTML内容中的高相关片段。
    
    •	**冗余信息过滤**：剔除广告、导航内容等无关信息，确保生成模型的输入高度精准。
    
    •	**避免幻觉内容**：当HTML文件中无相关信息时，EXTRACTOR-LLM 输出“None”，避免生成错误或无意义的内容。
    
    - **(1) 使用有限状态机（FSM）分割内容**
        
        •	**为何使用 FSM**：HTML文件中的内容格式复杂，包含段落、超链接、标点等多种结构。FSM 能有效解析这些结构，将内容切分为逻辑完整的片段。
        
        •	**具体实现**：
        
        1.	**解析 HTML DOM**：将 HTML 文件转化为结构化的 DOM 树。
        
        2.	**句子级切分**：根据标点符号（如句号、问号、感叹号）分割内容，同时避免误判：
        
        •	区分句号 “.” 和小数点（如“3.14”）。
        
        •	识别缩写词中的句号（如“Dr.”）。
        
        3.	**添加标记**：为每个分段内容添加唯一标签（TAG），用于后续引用。
        
    - 2.	**相关性分析**：
        
        •	**EXTRACTOR-LLM** 结合用户请求和分段内容，识别哪些内容片段与请求最相关。
        
        •	**标签输出**：EXTRACTOR-LLM 输出相关内容的标签（TAG）。
        
        •	如果相关性得分高于阈值（例如0.7），则将片段标记为相关。
        •	如果所有片段得分都低于阈值，则返回“None”。避免生成幻觉内容。
        
    - 3.	**高效优化**：，仅返回相关片段的标签而非完整内容，从而减少冗余信息传递。
        
        **(1) 提取标签而非全文**
        
        •	在实际输出中，仅返回相关片段的标签而非完整内容，从而减少冗余信息传递。
        
        •	提高后续生成模型的效率：
        
        •	生成模型（GENERATIVE-LLM）只需处理相关片段的标签对应内容，减少输入token数量。
        
        **(2) 使用轻量化模型**
        
        •	**模型规模优化**：
        
        •	EXTRACTOR-LLM 的9B参数规模远小于生成模型（如72B），推理速度更快。
        
        •	**优势**：
        
        •	高效处理大批量HTML文件。
        
        •	在真实场景中表现出较低的延迟，适用于在线任务。
        
    
    **5. 生成响应（Response Generation）**
    
    **核心组件：GENERATIVE-LLM**
    
    •	**功能**：结合用户请求和提取的相关信息生成最终输出。
    
    1.	**融合上下文**：将用户请求与EXTRACTOR-LLM提取的内容组合形成扩展Prompt。
    
    2.	**生成响应**：
    
    •	扩展Prompt传递给主生成模型（GENERATIVE-LLM），生成基于上下文的高质量响应。
    
    •	生成模型会参考EXTRACTOR-LLM的提取结果，避免处理过多无关内容。
    
    3.	**输出最终结果**：模型生成完成后，直接返回用户期望的内容。
    
- **Train**
    - **训练 PARSER-LLM 的指令集**
        
        YI-6B-CHAT：YI-6B-CHAT has been supervised fine-tuned to obtain basic instruction-following capabilities
        
        BGE-M3： 根据提取的关键词评估搜索结果的相关性并生成排名分数。
        
        **gpt4o标注**
        
        **Instruction Set for PARSER-LLM**包含约50,000条标注样本，具体分布如下：
        
        **1.真实请求数据**：约80%，主要来自 从01AI的实际生产环境中，采集一周内的API请求数据。这些请求是用户提交的真实问题，
        
        **2.人工生成补充**：约20%，用于覆盖特殊场景和增加多样性。
        
        时间敏感型问题：如“明天的天气预报”或“上周科技行业的动态”。
        
        多语言场景：如“2024年 AI market 的趋势”或“最新的智能锁市场状况如何？”
        
        为了避免模型对未见过的词汇（如新品牌名或专有名词）出现误判，指令集中加入了模拟的“新词”场景。
        
        **3.语言分布**：中文：60%，英文：30%，中英混合：10%
        
    
    **PARSER-LLM**
    
    •	**基础模型**：YI-6B-CHAT
    
    •	**模型背景**：YI-6B-CHAT 是在 YI-6B-BASE 基础上微调而来的，具有基本的指令跟随能力。
    
    •	**参数规模**：6B（60亿参数）。
    
    •	**训练方式**：使用一个包含多任务指令的训练集（如关键词提取、多语言支持）进行微调，使其能够同时完成“是否需要搜索”和“关键词提取”两项任务。
    
    •	**优化目标**：负对数似然（Negative Log-Likelihood, NLL），通过最小化生成正确关键词的损失函数来优化模型性能。
    
    EXTRACTOR-LLM 的训练数据集构建侧重于提高模型从HTML内容中提取相关信息的能力，
    
    •	**基础模型**：YI-9B-CHAT-16K
    
    •	**模型背景**：YI-9B-CHAT 是 YI-9B-BASE 模型的指令微调版本，支持16K长上下文处理能力。
    
    •	**参数规模**：9B（90亿参数）。
    
    •	**训练阶段**：
    
    1.	**监督微调阶段（Supervised Fine-Tuning, SFT）**：
    
    •	目标是训练模型具备从HTML文件中准确提取相关信息并标注TAG的能力。
    
    2.	**直接偏好优化阶段（Direct Preference Optimization, DPO）**：
    
    •	在EXTRACTOR-LLM的基础上，通过优质和低质提取对比样本进一步优化模型的偏好能力。
    
    •	**优化目标**：
    
    •	在SFT阶段，使用NLL最小化损失。
    
    •	在DPO阶段，引入强化学习损失函数，通过 LLM 或人工评价的偏好优化提取结果。
    
    - **Instruction Set for EXTRACTOR-LLM**
        
        最终构建了约60,000条标注样本，具体分布如下：
        
        **1. 真实请求数据**：约70%，主要来自 01AI 的实际生产环境中，随机抽取了一周内的用户请求和对应的HTML返回内容。这些内容包括：
        
        用户问题与返回内容的配对，如“推荐三款最好的智能门锁”，返回内容涵盖品牌描述、产品特点和市场价格。
        
        使用有限状态机（FSM）对HTML文件进行分段标注，为每个句子生成对应的标签（TAG），例如：
        
        <TAG-1> 小米门锁售价799元，非常实惠！ </TAG-1>
        
        <TAG-2> 德施曼V9配备了德国Hiyie指纹算法芯片。 </TAG-2>
        
        **2. 人工生成补充**：约30%，用于覆盖真实数据不足的场景，特别是复杂任务和多样化请求。
        
        •	**多任务情境**：例如，输入“2024年市场份额最大的智能门锁品牌有哪些？”，生成包含不同段落的HTML内容，并标注相关标签。
        
        •	**负样本生成**：设计完全无关的HTML内容，例如搜索“华为门锁”时，返回完全是“小米门锁”的内容，标注标签为空。
        
        •	**复杂上下文**：加入需要多步推理的内容，例如从多个段落提取所有相关信息，标注模型所需的多个标签。
        
        **3. 语言分布**：中文：65%，英文：25%，中英混合：10%。
        
        **DPO（Direct Preference Optimization）数据集**
        
        DPO 数据集主要用于强化 EXTRACTOR-LLM 的输出，优化模型对高质量提取的偏好能力。该数据集包含约40,000条对比标注样本，具体分布如下：
        
        **1. 真实请求数据**：约50%，来自实际生产环境中用户请求和搜索引擎返回的HTML文件。
        
        从每个HTML文件中提取原始标签内容，并利用 GPT-4O 或人工调整生成优质内容与低质内容的对比样本。例如：
        
        •	**优质内容**：精确提取描述某产品的核心特点。
        
        •	**低质内容**：包含过多冗余或与问题无关的信息。
        
        **2. 人工生成补充**：约50%，通过人工设计与 GPT-4O 生成相结合，扩展数据多样性。
        
        **对比样本生成**：针对相同的用户请求，生成不同的提取结果（一个高质量，一个低质量），并通过 LLM 或人工标注对其进行打分。
        
        **替换测试**：从HTML内容中随机选择部分段落进行替换，测试模型的内容提取稳定性和鲁棒性。
        
        **3. 语言分布**：中文：60%，英文：30%，中英混合：10%。
        
    
    GENERATIVE-LLM 的训练数据集旨在优化生成式模型的上下文整合能力，以及基于提取内容生成高质量响应的能力。
    
    - **Instruction Set for GENERATIVE-LLM**
        
        数据集包含约70,000条标注样本，具体分布如下：
        
        **1. 真实请求数据**：约60%，主要来源于01AI的生产环境，涵盖真实用户的查询请求与HTML返回内容。
        
        **用户问题与内容匹配**：采集真实用户请求（如“最新的智能锁推荐”），结合搜索引擎返回的网页内容，生成带有上下文的生成任务。例如：
        
        输入：用户请求 + EXTRACTOR-LLM 提取的内容标签（TAG）。
        
        输出：生成一段连贯、准确的回答，如“德施曼V9是一款领先的智能门锁，配备德国Hiyie指纹算法芯片。”
        
        **2. 人工生成补充**：约40%，用于扩充多样化和复杂场景的覆盖度。
        
        •	**高低质量对比**：
        
        •	构建优质和低质回答的对比任务，例如：
        
        •	优质回答：准确提取与用户问题相关的信息，并生成结构化答案。
        
        •	低质回答：包含无关内容或模糊表述，如遗漏关键信息或生成片段化答案。
        
        **多轮对话**：设计多轮交互任务，测试模型在多层次上下文中的生成效果。例如：
        
        •	用户：推荐2024年的智能门锁。
        
        •	模型：推荐包括小米门锁799元、德施曼V9等，后者配备先进的算法芯片。
        
        **3. 语言分布**：中文：55%，英文：35%，中英混合：10%。
        
    
    •	**基础模型**：
    
    1.	**QWEN2-72B**（开源）：
    
    2.	**YI-34B**（开源）：
    
    3.	**YI-LARGE**（闭源）：
    
    •	**训练阶段**：
    
    •	使用高质量的上下文-生成对样本进行**微调**，特别是基于 EXTRACTOR-LLM 提取的内容生成丰富且精准的回答。
    
    •	引入对比学习（**Contrastive Learning**），通过优质与低质生成样本对比来优化模型生成质量。
    
    •	**目标优化**：
    
    •	使用 GPT-4O 作为评价基准，微调模型以提升对实际问题的回答质量，同时控制生成内容的冗余和幻觉现象（Hallucination）。
    
- Eval
    
    Benchmark
    
    评估数据集是根据真实世界的数据构建的。
    
    我们从 01AI 的真实 API 请求中随机抽样了 500 个查询——我们确认这个评估数据集与我们用来准备第 4.2 节中介绍的指令集的原始数据之间没有重叠。重复数据删除后 y — 仍有 463 个唯一请求，并附有从搜索引擎返回的相应网页中抓取的内容。
    
    WIN（胜）、TIE（平）、LOSE（负）及 P-BIA（两次评估不一样）。
    
    Base：基础任务评估的 F1 分数，衡量模型在简单检索任务中的准确性。
    Reasoning：推理任务的 F1 分数，测试模型从多段内容中提取并推理答案的能力。
    Multi-Ans：多答案任务的 F1 分数，测试模型是否能够提取出所有与问题相关的正确答案。
    Multi-Q：多问题任务的 F1 分数，测试模型是否能够在一次推理中同时回答多个问题，并给出与问题对应的所有答案。
    
    两次评估中产生一致结果的回答被接受，而结果不一致的回答被归类为特殊位置偏差 （P-BIA） 类别。
    
    - **1. 端到端性能评估**
        
        •	**评估目标**：评估模型在生成内容质量和生成成本方面的表现。
        
        •	**数据集**：从01AI生产环境中随机抽取500条真实API请求，并配以相应的HTML检索内容，经过去重后保留463条独立请求。
        
        • Compare Method
        
        1.	**Internet-SAG-Ext**（本文提出的框架）。
        
        2.	**Internet-SAG-Naive**（未包含EXTRACTOR-LLM，直接将HTML内容输入生成模型）。
        
        3.	**VectorDB-RAG**（传统RAG框架，使用矢量数据库检索固定语料）。
        
        **生成模型**：对比不同规模的生成模型（如 QWEN2-72B、YI-34B、YI-LARGE）。
        
        **评估指标**：生成内容质量 and 生成成本
        
        生成内容质量：使用 GPT-4O 对生成结果进行评分，包括 WIN（胜）、TIE（平）、LOSE（负）及 P-BIA（位置偏置）。
        
        生成成本：统计生成模型消耗的输入Token数量。
        
        **实验结果**：
        
        •**质量评估**：Internet-SAG-Ext 在大部分场景下生成内容质量优于基线方法，尤其是在过滤无关信息和提高上下文相关性方面表现突出。
        
        如在使用 QWEN2-72B 时，Internet-SAG-Ext 对 VectorDB-RAG 的 WIN 比例为 258:66，显示显著优势。
        
        **成本评估**：Internet-SAG-Ext 在Token使用量上比 VectorDB-RAG 和 Internet-SAG-Naive 分别降低了21%和47%，展现出高效的生成性能。
        
    - **2.EXTRACTOR-LLM**
        
        **评估目标**：评估 EXTRACTOR-LLM 的上下文提取准确性、标签定位精度及拒绝无关内容的能力。
        
        **评估基准**：
        
        1.**Synthetic Benchmark**：基于控制变量的合成数据，评估多跳推理、多问题回答等任务。
        
        2.**Open-source Benchmark**：来自开源数据集（如 Multi-hop QA 和 MulTiple），测试模型在多答案提取和复杂推理任务中的表现。
        
        3.**Real-world Benchmark**：从生产环境抽取500条真实请求，用于测试模型的实际应用能力。
        
        **评估指标**：采用 Precision、Recall 和 F1 评分等多标签分类指标，此外还引入 Exact Match（完全匹配率）来评估对无关内容的拒绝能力。
        
        **实验结果**：
        
        在 Synthetic Benchmark 中，EXTRACTOR-LLM 的表现接近 GPT-4O，在多问题任务中达到完全匹配（F1=1.0），展现了强大的上下文提取能力。
        
        •	在 Open-source Benchmark 中，EXTRACTOR-LLM 在推理任务和多答案任务中分别以 0.724 和 0.581 的 F1 分数超越了所有基线模型。
        
        •	在 Real-world Benchmark 中，EXTRACTOR-LLM 在拒绝无关内容（Exact Match=0.7985）方面表现出色，比 GPT-4O 提升了4个百分点，同时在 F1 分数上也略胜一筹（0.7698 vs 0.7431）。
        
    
    **Base**：基础任务评估的 F1 分数，衡量模型在简单检索任务中的准确性。
    
    **Reasoning**：推理任务的 F1 分数，测试模型从多段内容中提取并推理答案的能力。
    
    **Multi-Ans**：多答案任务的 F1 分数，测试模型是否能够提取出所有与问题相关的正确答案。
    
    **Multi-Q**：多问题任务的 F1 分数，测试模型是否能够在一次推理中同时回答多个问题，并给出与问题对应的所有答案。