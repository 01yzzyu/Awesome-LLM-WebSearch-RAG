# RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation

7B Llama2 

- **Motivation（研究动机）**

  依赖于静态的大规模预训练数据，难以动态更新，导致它们容易在处理复杂或模糊查询时生成错误甚至幻觉性（hallucinatory）的回答。

  为了克服这些问题，检索增强生成（Retrieval-Augmented Generation, RAG）通过将外部文档引入生成过程，有效结合模型的上下文学习能力与非参数知识。

  然而，现有的RAG方法往往仅聚焦于初始输入查询，忽视了复杂或模糊查询在检索和回答过程中需要进一步澄清或分解的可能性。

  为了解决上述问题，RQ-RAG提出了一种**改进查询生成的机制**，旨在通过**显式的重写、分解和消歧操作提升复杂任务的检索与生成能力**。实验结果表明，RQ-RAG在单跳和多跳问答任务中相较于之前的SOTA方法取得了显著的性能提升。

- **Pipeline（工作流程）**

  RQ-RAG的核心流程主要包含以下几个阶段：

  1.	**数据集构建**：

  •数据集基于三类查询（重写、分解和消歧）进行扩展，利用大规模预训练模型（如ChatGPT）生成经过改写后的搜索查询，并检索相关上下文。

  •对检索得到的上下文进行答案生成并迭代优化，最终得到40k条多样化的数据实例。

  2.**模型训练**：

  •使用标准自回归方式对构建好的数据集进行训练。模型目标函数是最大化在给定上下文和改写后的查询条件下生成正确答案的概率。

  •利用特殊标记（control tokens）指导生成过程，确保模型能够动态选择适合的操作（如重写、分解、消歧或直接回答）。

  3.	**采样策略**：

  •**基于困惑度（PPL）**：选择生成困惑度最低的路径。

  •**基于信心度**：选择最终答案置信度最高的路径。

  •**集成方法**：综合不同路径的置信度，选择得分最高的答案。

  •**系统上界分析**：分析所有可能路径是否包含正确答案，评估系统潜力。

  4.	**推理过程**：

  •在推理过程中，模型对复杂查询进行迭代操作，包括重写、分解、或消歧。通过多轮生成和检索，最终综合上下文生成答案。

  5.**评价任务**：

  Single-Hop QA ****：Arc-Challenge, PopQA  and OpenbookQA

  Multi-Hop QA ：HotpotQA, 2WikiMultiHopQA  and Musique 

  •采用准确率和F1分数评估模型性能。

  总结来看，RQ-RAG通过精细化的数据集构建、动态查询优化以及高效的采样策略，大幅度提高了复杂问答场景中的模型表现。

- Train

  **数据集构建**

  RQ-RAG 的数据集构建流程旨在模拟推理时的实际操作步骤，从而提高模型在处理复杂查询和生成高质量答案方面的能力。具体步骤如下：

  1.	**数据来源与任务分类**：

  •包含三类主要任务：需要**重写**的查询、需要**分解**的复杂多跳查询，以及需要**消歧**的模糊查询。

  •数据来源包括多种问答数据集，如 Arc-Challenge（单跳问答）、HotpotQA（多跳问答）以及 OpenbookQA（知识问答）。

  •数据任务被分为单跳问答、多跳问答、和模糊查询三种类型，分别进行针对性处理。

  2.	**查询改写与检索上下文生成**：

  •	使用 ChatGPT 自动化标注，首先对初始查询进行改写、分解或消歧操作。例如：

  •	对于模糊查询，ChatGPT 根据上下文生成更清晰的目标查询。

  •	对于多跳查询，模型将复杂问题分解为多个可回答的子问题。

  •根据改写后的查询，通过 DuckDuckGo 或其他检索引擎获取相关上下文文档。每次检索返回的结果中，包含标题、摘要和相关网页内容。

  3.	**生成新答案**：

  •ChatGPT 根据检索到的上下文生成基于内容的答案，而非简单复用原始数据集中提供的答案。

  这种方法避免了上下文与答案之间的语义不匹配问题，同时提高了生成答案的上下文关联性。

  •每个数据实例最终包含：初始查询、特定操作（如重写、分解、或消歧）、改写后的查询、检索上下文，以及最终生成的答案。

  4.	**数据规模与特性**：

  •	最终构建了约40,000条数据实例，涵盖广泛的问答场景。任务示例包括多轮对话、多跳推理和模糊问题处理。

  •	数据中加入了特殊标记（control tokens），用于标识操作类型及模型所需的步骤，这些标记在训练时指导模型动态执行任务。

  整个数据集通过结合自动化标注与人工优化设计，确保了高质量的多样化数据覆盖，能够支持模型在复杂任务场景中的泛化能力。

  **模型训练 7B Llama2** 

  在构建完数据集后，RQ-RAG 模型通过标准自回归方式进行训练，旨在让模型具备动态处理复杂查询的能力。具体训练过程如下：

  1.	**训练目标与机制**：

  •模型目标函数是最大化在特定上下文条件下生成正确答案的概率：

  其中， 为改写的查询， 为检索到的上下文， 为原始输入。

  •模型在训练时不仅需要生成答案，还需要动态选择适合的步骤（如重写、分解或直接回答），并迭代生成中间查询与答案。

  2.	**特殊标记与动态任务执行**：

  •数据集中引入的特殊标记（control tokens）用来指导模型的生成过程。例如，当模型检测到模糊查询时，会选择“消歧”操作标记并生成改写后的查询；对于复杂问题，则选择“分解”操作并生成多个子查询。

  •模型在每个步骤动态调整生成路径，形成多轮迭代的生成和检索过程。

  3.	**采样策略**：

  •	训练完成后，模型需要在推理过程中选取最优路径。为了优化路径选择，提出了三种采样策略：

  •	**困惑度（PPL）最小路径**：选择整体生成困惑度最低的路径，确保回答的语言流畅性和一致性。

  •	**置信度最高路径**：选择最终答案置信度最高的路径，更关注答案的准确性。

  •	**集成策略**：结合所有路径的置信度评分，选择综合得分最高的路径。

  •	此外，通过评估系统的上界（upper bound），发现如果模型能够完美选择最佳路径，其成功率可以显著提高，展示了模型的潜力。

  4.	**多任务与多样化训练**：

  •	模型训练结合了单跳问答、多跳问答以及模糊查询处理，确保其在多场景下都具备优秀表现。

  •	在训练过程中，模型动态平衡“改写”、“分解”、“消歧”和“直接回答”四种操作，形成统一高效的推理和生成框架。

  通过这一训练流程，RQ-RAG 模型成功实现了对复杂查询的动态处理和回答生成能力，特别是在需要多跳推理和模糊消歧的场景中表现尤为突出。

- Eval

  1.**单跳问答任务**：

  •**Arc-Challenge**：包含1172个四选一问答问题，问题主要考察复杂推理能力，使用准确率（Accuracy）作为主要评价指标。

  •**PopQA**：包含1399个长尾知识问答实例，评估模型生成答案是否包含参考答案，采用匹配评分（Match Score）。

  •**OpenbookQA**：包括500个四选一问答实例，问题基于开放知识领域，使用准确率作为指标。

  2.**多跳问答任务**：

  •**HotpotQA**：测试问题需要通过多个文档的综合推理进行回答，每个问题对应2个相关文档和8个干扰文档，采用F1分数作为评价指标。

  •**2WikiMultiHopQA**：问题需要从多个候选文档（2-4个相关文档）中提取关键信息，结合上下文进行多跳推理，采用F1分数评价性能。

  •**MuSiQue**：问题具有更高复杂度，每个问题需要从20个候选文档中进行2至4跳推理，考察模型在高度复杂任务中的性能，使用F1分数评估。

  **2. 基线模型对比**

  RQ-RAG与多个基线方法进行对比，包括无检索模型、检索增强模型以及现有的SOTA方法。

  1.	**无检索基线**：

  •	**Llama2-7B Zero-Shot**：直接对问题进行回答，无外部上下文支持。

  •	**Llama2-7B Zero-Shot（Chat版）**：增强对话版本，改进回答流畅性。

  •	**Llama2-7B（特定任务微调）**：基于单跳或多跳任务数据微调模型。

  •	**Llama2-7B（无检索增强）**：在没有中间检索步骤的情况下，使用构建的数据集进行微调。

  2.	**检索增强基线**：

  •	**SAIL-7B**：一种通过学习过滤无关上下文的检索增强模型。

  •	**Self-RAG-7B**：结合自反思策略进行上下文过滤和答案生成的SOTA方法。

  •	**其他方法**：包括基于链式推理（Chain-of-Thought）和自我提问（Self-Ask）的方法。
