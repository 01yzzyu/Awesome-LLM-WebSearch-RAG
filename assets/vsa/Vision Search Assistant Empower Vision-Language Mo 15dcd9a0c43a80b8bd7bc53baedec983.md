# Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines

- **Motivation（研究动机）**
    
    传统搜索引擎和大规模视觉语言模型（VLM）在处理复杂视觉信息时存在局限性。虽然大规模语言模型（LLM）在开放领域的知识问答中表现出强大的能力，但它们对超出训练数据范围的新图像或事件表现出较低的泛化能力。这主要是因为：
    
    1.	**知识截止问题**：VLM 和 LLM 的知识依赖于训练数据的时间节点，对于新事物或新事件，它们的回答往往不准确或缺乏信息。
    
    2.	**视觉信息处理不足**：现有基于网络的增强生成模型（例如 Retrieval-Augmented Generation, RAG）主要集中在文本数据上，对视觉内容的理解和处理能力不足，尤其在用户输入的是未见过的图像或需要图文结合的场景时。
    
    3.	**更新成本高昂**：频繁更新 VLM 模型以获取最新知识并不现实，因为这需要巨大的计算资源。
    
    为了克服这些限制，VSA 提出了一种将 VLM 和网络代理协同结合的全新框架。这一框架通过实时访问网络上的视觉和文本信息，使模型能够处理未见过的视觉内容并生成可靠的答案。这种协作式的框架为解决开放领域中的多模态检索增强生成任务提供了新的思路。
    
- **Pipeline**
    
    ![image.png](Vision%20Search%20Assistant%20Empower%20Vision-Language%20Mo%2015dcd9a0c43a80b8bd7bc53baedec983/image.png)
    
    **1. Visual Content Formulation（视觉内容表述）**
    
    该阶段旨在提取图像中的关键对象并生成其相关的文字描述：
    
    •使用开词汇检测器（Open-Vocab Detector）对图像进行区域识别，提取关键对象的区域。
    
    •针对每个区域，通过预训练的 VLM 根据用户输入生成区域级别的视觉描述。
    
    •将每个区域的描述与其他区域的描述进行关联，生成“相关表述”（Correlated Formulation），综合表示图像的整体语义。
    
    这种相关表述不仅关注单个对象的描述，还通过关联不同对象的信息，捕捉图像中对象之间的语义联系。
    
    **2. Web Knowledge Search（网络知识检索）**
    
    为了获得更全面的网络知识，VSA 引入了一个递进式搜索算法——“搜索链”（Chain of Search）：
    
    •**规划代理（Planning Agent）**：根据视觉相关表述生成子问题（Sub-Questions），这些子问题指向检索的核心内容。
    
    •**搜索代理（Searching Agent）**：通过网络代理检索这些子问题对应的内容，提取相关的网页信息，并使用 LLM 对检索结果进行筛选和总结。
    
    •通过多轮迭代扩展搜索链，在每次迭代中进一步生成新子问题，从而逐步构建一个有向图结构，表示从视觉内容到网络知识的递进推理过程。
    
    •检索过程中，系统不断评估是否已经获得足够的知识来回答用户问题，否则继续生成新的子问题。
    
    **3. Collaborative Generation（协作生成）**
    
    结合用户初始输入、视觉相关表述和从网络检索获得的知识，VSA 使用 VLM 生成最终答案。具体步骤包括：
    
    •将图像原始内容、用户问题、每个关键对象的相关表述以及网络检索知识输入模型。
    
    •模型通过协作生成综合考虑多模态信息的最终回答。
    
- Eval
    
    **1. 开放集评估（Open-Set Evaluation）**
    
    开放集评估旨在测试 VSA 处理从未见过的图像或事件时的泛化能力。
    
    •**数据来源**：构建了一个涵盖新闻、商品、事件和社交媒体内容的多模态数据集，其中包含 100 个图文对，涵盖 2024 年 7 月至 9 月的最新事件和概念。
    
    •**评估指标**：
    
    •**事实性**：由 10 位人类专家评估回答的准确性。
    
    •**相关性**：根据回答与问题的匹配程度进行评分。
    
    •**支持性**：回答是否包含充足的背景信息和逻辑支持。
    
    **2. 封闭集评估（Closed-Set Evaluation）**
    
    封闭集评估用于测试 VSA 在固定基准数据集上的表现，以对比其在特定任务中的性能。
    
    •**数据集**：使用 LLaVA-W 基准数据集，其中包含 60 个视觉语言任务，涵盖对话能力、细节描述能力以及推理能力。
    
    •	**对比基线**：
    
    •	LLaVA-1.6-7B 的标准模式和 “简单检索模式”（naive search mode，即 Google 图像搜索）。
    
    •	增强版 LLaVA-1.6-7B（引入搜索链模块）。
    
    **消融实验（Ablation Study）**
    
    为了验证各模块对整体性能的贡献，研究设计了多组消融实验：
    
    1.	**搜索内容优化（What to Search）**：
    
    •	比较基于图像整体描述与基于关键对象描述的效果，发现使用对象级描述可以避免视觉冗余，提高搜索精准度。
    
    2.	**搜索方式优化（How to Search）**：
    
    •	引入搜索链（Chain of Search），通过逐步生成和总结子问题，显著优于简单的 Google 搜索和重新排序策略。
    
    3.	**多对象场景优化（Complex Scenarios of Search）**：
    
    •	对于包含多个对象的复杂图像，加入视觉关联（Visual Correlation）显著提升了回答质量。
    
    **结果总结**
    
    评估结果显示，VSA 在开放集和封闭集场景下均显著优于现有的多模态模型。其模块化设计（如搜索链和视觉关联）能够高效整合视觉和文本信息，生成准确、相关且有支持性的答案。通过全面的评估，VSA 证明了其在解决新图像和跨模态任务中的强大能力，并展示了其在学术研究和实际应用中的潜力。